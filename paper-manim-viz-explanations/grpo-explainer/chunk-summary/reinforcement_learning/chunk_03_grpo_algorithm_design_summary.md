# GRPO (Group Relative Policy Optimization) - Comprehensive Educational Summary

## 1. Intuition & Core Idea

### What Problem Does GRPO Solve?

Imagine you're training a large language model to write better responses to questions. Traditional approaches like PPO (Proximal Policy Optimization) require training two separate neural networks:
1. A **policy network** that generates responses
2. A **value network** that estimates how good each generated response will be

This is like having two chefs in a kitchen - one who cooks and one who judges the food. But there's a catch: the judging chef needs to evaluate every single word as it's being written, not just the final dish!

### Why Is This Problematic?

1. **Computational Overhead**: Training two large networks doubles memory usage and computational requirements
2. **Misalignment with Reward Models**: Modern reward models typically only score complete responses, not individual words during generation
3. **Training Instability**: Estimating values for intermediate tokens introduces noise and complexity

### GRPO's Clever Solution

GRPO eliminates the need for a separate value network entirely. Instead of trying to predict future rewards, it uses a much simpler approach:

**Group-Based Baseline**: For each question, generate multiple responses (say 4-8), then use the **average reward** of all these responses as a baseline. Responses that score above average get rewarded; those below get penalized.

Think of it like this: instead of hiring a food critic to judge each dish individually, you let people taste-test multiple dishes and compare them relatively. The "average deliciousness" becomes your benchmark.

### Key Advantages:
- **No extra value network** â†’ 50% memory savings
- **Works naturally with existing reward models** that score complete responses
- **More stable training** due to simpler advantage calculation
- **Leverages comparative nature** of human preference data

## 2. Technical Deep Dive

### Mathematical Formulation

Let me break down the GRPO objective function step by step:

$$
\footnotesize
\begin{split}
    \mathcal{J}_{GRPO}(\theta) &= \mathbb{E}{[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)]}  \\
    & \frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left\{ \min \left[ \frac{\pi_\theta(o_{i,t} | q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t} | q, o_{i,<t})} \hat{A}_{i,t}, \text{clip} \left( \frac{\pi_\theta(o_{i,t} | q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t} | q, o_{i,<t})}, 1 - \epsilon, 1 + \epsilon \right)  \hat{A}_{i,t} \right] - \beta \mathbb{D}_{KL}\left[\pi_{\theta} || \pi_{ref}\right]\right\}
\end{split}
$$

### Breaking Down the Components:

#### Variables and Notation:
- $q$: Input question/prompt
- $o_i$: Generated response (output sequence) for the $i^{th}$ sample
- $G$: Number of responses generated per question (group size)
- $\pi_\theta$: Current policy network with parameters $\theta$
- $\pi_{\theta_{old}}$: Old/frozen policy network (for importance sampling)
- $\pi_{ref}$: Reference policy (typically pre-trained model)
- $o_{i,<t}$: Tokens generated before position $t$ in response $i$
- $\epsilon, \beta$: Hyperparameters for clipping and KL regularization

#### Key Terms:

1. **Policy Ratio**: 
   $$r_{i,t} = \frac{\pi_\theta(o_{i,t} | q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t} | q, o_{i,<t})}$$
   
   This measures how much more likely the current policy is to generate token $o_{i,t}$ compared to the old policy.

2. **Clipped Objective** (PPO-style):
   $$\min[r_{i,t} \hat{A}_{i,t}, \text{clip}(r_{i,t}, 1-\epsilon, 1+\epsilon) \hat{A}_{i,t}]$$
   
   This prevents large policy updates by clipping the ratio between $(1-\epsilon)$ and $(1+\epsilon)$.

3. **Advantage Calculation** $\hat{A}_{i,t}$:
   
   Unlike PPO which uses value networks, GRPO computes advantages relative to the group mean:
   $$\hat{A}_{i,t} = r_i - \bar{r}$$
   
   Where $r_i$ is the reward for response $i$, and $\bar{r} = \frac{1}{G}\sum_{j=1}^G r_j$ is the average reward across the group.

4. **KL Regularization**:
   $$\mathbb{D}_{KL}\left[\pi_{\theta} || \pi_{ref}\right] = \frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})} - \log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})} - 1$$
   
   This ensures the learned policy doesn't deviate too much from a reference model.

### Why This Works Better:

1. **No Value Network**: Eliminates the complex task of estimating per-token values
2. **Natural Alignment**: Uses full-response rewards that align with how reward models work
3. **Stable Training**: Group-based baselines provide more stable advantage estimates
4. **Efficient**: Reduces computational overhead while maintaining performance

## 3. Code Implementation Walkthrough

Let me walk through the key components of the GRPO implementation:

### Main Loss Computation (`_compute_loss`)

```python
def _compute_loss(self, model, inputs):
    # Step 1: Prepare inputs and compute log probabilities
    prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
    completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
    input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
    
    # Compute per-token log probabilities for the completion
    per_token_logps, entropies = self._get_per_token_logps_and_entropies(...)
    
    # Step 2: Compute KL divergence (if regularization enabled)
    if self.beta != 0.0:
        ref_per_token_logps = inputs["ref_per_token_logps"]
        per_token_kl = (
            torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
        )
    
    # Step 3: Compute importance sampling ratios
    old_per_token_logps = inputs.get("old_per_token_logps", per_token_logps.detach())
    log_ratio = per_token_logps - old_per_token_logps
    
    # Apply clipping based on importance sampling level (token vs sequence)
    coef_1 = torch.exp(log_ratio)
    coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)
    
    # Step 4: Apply PPO-style clipped objective
    advantages = inputs["advantages"]  # Pre-computed group-relative advantages
    per_token_loss1 = coef_1 * advantages.unsqueeze(1)
    per_token_loss2 = coef_2 * advantages.unsqueeze(1)
    per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
    
    # Step 5: Add KL regularization
    if self.beta != 0.0:
        per_token_loss = per_token_loss + self.beta * per_token_kl
    
    # Step 6: Aggregate loss
    loss = ((per_token_loss * completion_mask).sum(-1) / completion_mask.sum(-1).clamp(min=1.0)).mean()
    
    return loss
```

### Reward and Advantage Calculation

The `_calculate_rewards` function handles the core GRPO innovation:

```python
def _calculate_rewards(self, inputs, prompts, completions, completion_ids_list):
    # Step 1: Compute rewards for each completion using reward models
    rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)
    
    for i, reward_func in enumerate(self.reward_funcs):
        # Apply reward function to (prompt + completion) pairs
        rewards_per_func[:, i] = reward_func(prompts + completions)
    
    # Step 2: Normalize rewards by group mean (the key GRPO innovation)
    # Each group consists of G responses to the same prompt
    # Advantages = rewards - group_mean_rewards
    return rewards_per_func
```

### Log Probability Computation

The `_get_per_token_logps_and_entropies` function efficiently computes token probabilities:

```python
def _get_per_token_logps_and_entropies(...):
    # Process inputs in batches to save memory
    all_logps = []
    for start in range(0, input_ids.size(0), batch_size):
        # Forward pass through model
        logits = model(input_ids_batch, attention_mask_batch).logits
        
        # Compute log probabilities for actual tokens that were generated
        completion_ids = input_ids_batch[:, -logits_to_keep:]
        logps = selective_log_softmax(logits, completion_ids)
        all_logps.append(logps)
    
    return torch.cat(all_logps, dim=0), entropies
```

## 4. Worked Example

Let me provide a concrete example with actual numbers:

### Setup:
- Question: "What is the capital of France?"
- Group size (G): 3 responses per question
- Sequence length: 4 tokens per response

### Step 1: Generate Responses
```
Response 1: "Paris is the capital"     (Reward: 0.8)
Response 2: "The capital is Paris"     (Reward: 0.9)  
Response 3: "France capital is Lyon"   (Reward: 0.3)  # Incorrect
```

### Step 2: Calculate Group Mean and Advantages
```
Group Mean = (0.8 + 0.9 + 0.3) / 3 = 0.67

Advantages:
- Response 1: 0.8 - 0.67 = 0.13
- Response 2: 0.9 - 0.67 = 0.23  
- Response 3: 0.3 - 0.67 = -0.37
```

### Step 3: Compute Policy Ratios
For simplicity, let's look at the first token of each response:

```
Token: "Paris" (Response 1)
Ï€_old("Paris"|question) = 0.7
Ï€_new("Paris"|question) = 0.8
Ratio = 0.8/0.7 = 1.14

Token: "The" (Response 2)  
Ï€_old("The"|question) = 0.6
Ï€_new("The"|question) = 0.65
Ratio = 0.65/0.6 = 1.08

Token: "France" (Response 3)
Ï€_old("France"|question) = 0.4
Ï€_new("France"|question) = 0.35
Ratio = 0.35/0.4 = 0.875
```

### Step 4: Apply Clipped PPO Objective
With Îµ = 0.2 (clipping range [0.8, 1.2]):

```
Response 1 Token 1:
min(1.14 Ã— 0.13, clip(1.14, 0.8, 1.2) Ã— 0.13) = min(0.148, 1.14 Ã— 0.13) = 0.148

Response 2 Token 1:
min(1.08 Ã— 0.23, clip(1.08, 0.8, 1.2) Ã— 0.23) = min(0.248, 1.08 Ã— 0.23) = 0.248

Response 3 Token 1:
min(0.875 Ã— (-0.37), clip(0.875, 0.8, 1.2) Ã— (-0.37)) = min(-0.324, 0.875 Ã— (-0.37)) = -0.324
```

### Step 5: Add KL Regularization
If Î² = 0.1 and KL divergence = 0.05:
```
Final Loss Terms:
- Response 1: 0.148 + 0.1 Ã— 0.05 = 0.153
- Response 2: 0.248 + 0.1 Ã— 0.05 = 0.253  
- Response 3: -0.324 + 0.1 Ã— 0.05 = -0.319
```

### Interpretation:
- Response 2 (best answer) gets highest positive update encouragement
- Response 3 (incorrect) gets negative update (discouraged)  
- Response 1 gets moderate positive update

## 5. Mathematical Derivation

### KL Divergence Approximation Derivation

The paper mentions using an unbiased estimator for KL divergence:

$$\mathbb{D}_{KL}\left[\pi_{\theta} || \pi_{ref}\right] = \frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})} - \log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})} - 1$$

**Derivation:**

Start with the definition of KL divergence:
$$D_{KL}(P||Q) = \mathbb{E}_P\left[\log\frac{P(X)}{Q(X)}\right]$$

Using importance sampling with distribution Q:
$$D_{KL}(P||Q) = \mathbb{E}_Q\left[\frac{P(X)}{Q(X)} \log\frac{P(X)}{Q(X)}\right]$$

Let $r(X) = \frac{P(X)}{Q(X)}$, so:
$$D_{KL}(P||Q) = \mathbb{E}_Q[r(X) \log r(X)]$$

The key insight is using the inequality $\log x \leq x - 1$ with equality at $x=1$. Consider:
$$f(r) = r \log r - r + 1$$

This function has minimum value 0 at $r=1$, and is always non-negative. Therefore:
$$r \log r = (r - 1) + (r \log r - r + 1)$$

Taking expectations under Q:
$$\mathbb{E}_Q[r \log r] = \mathbb{E}_Q[r - 1] + \mathbb{E}_Q[r \log r - r + 1]$$

Since $D_{KL}(P||Q) = \mathbb{E}_Q[r \log r]$:
$$D_{KL}(P||Q) = \mathbb{E}_Q[r - 1] + \mathbb{E}_Q[r \log r - r + 1]$$

Rearranging:
$$D_{KL}(P||Q) = \mathbb{E}_Q[r - \log r - 1]$$

This gives us the unbiased estimator used in GRPO.

### Why This Estimator is Useful:

1. **No gradient required**: Can be computed without backpropagating through the reference model
2. **Always positive**: Guarantees non-negative KL estimates
3. **Unbiased**: Correct in expectation even with finite samples
4. **Computationally efficient**: Only requires probability ratios

## 6. Key Takeaways

### ðŸŽ¯ Most Important Points:

1. **Eliminates Value Networks**: GRPO removes the need for separate value function approximation, cutting memory usage by ~50%
2. **Group-Based Advantages**: Uses relative rewards within groups rather than absolute value estimates
3. **Natural Reward Model Alignment**: Works seamlessly with existing reward models that score complete responses
4. **Stable Training**: Simpler advantage calculation leads to more stable policy optimization

### âš ï¸ Common Pitfalls & Misconceptions:

1. **Not Just PPO Without Value Net**: GRPO fundamentally changes how advantages are computed - it's not simply removing the value network from PPO
2. **Group Size Matters**: Too small groups lead to noisy baselines; too large groups waste computational resources
3. **KL Regularization is Critical**: Without proper regularization, policies can diverge dramatically from reference models
4. **Importance Sampling is Essential**: Using old policy probabilities prevents catastrophic updates during training

### ðŸ”§ Practical Implementation Tips:

1. **Batch Processing**: Process responses in groups to enable efficient group-mean calculations
2. **Memory Management**: Use techniques like `logits_to_keep` to avoid computing unnecessary logits
3. **Numerical Stability**: Implement proper clipping and gradient scaling to prevent instability
4. **Monitoring**: Track clip ratios and KL divergences to diagnose training issues

### ðŸ“š Further Reading:

1. **Original PPO Paper**: Schulman et al., "Proximal Policy Optimization Algorithms" (2017)
2. **RLHF Fundamentals**: Christiano et al., "Deep Reinforcement Learning from Human Preferences" (2017)  
3. **Mixture of Experts**: Fedus et al., "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity" (2021)
4. **Advanced KL Methods**: Tomczak, "Learning Model-based Planning from Scratch" (2017)

### ðŸ”„ Related Concepts:

- **REINFORCE with Baseline**: Classical policy gradient method that also uses baselines
- **A3C/A2C**: Actor-Critic methods that simultaneously learn policies and values
- **Importance Sampling**: Statistical technique for estimating properties of distributions
- **Contrastive Learning**: Learning by comparing positive and negative examples (similar spirit to group-relative rewards)

GRPO represents a significant advancement in efficient reinforcement learning for language models, offering a practical compromise between performance and computational efficiency that makes large-scale RLHF more accessible.