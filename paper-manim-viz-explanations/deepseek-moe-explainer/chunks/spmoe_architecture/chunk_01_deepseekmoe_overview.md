# DeepSeekMoE Overview

On top of the generic MoE architecture outlined in Section (ref: sec:preliminary), we introduce DeepSeekMoE, which is specifically designed to exploit the potential of expert specialization. 
As illustrated in Figure (ref: fig:deepseek_moe), our architecture incorporates two principal strategies: fine-grained expert segmentation and shared expert isolation. 
Both of these strategies are designed to elevate the level of expert specialization.